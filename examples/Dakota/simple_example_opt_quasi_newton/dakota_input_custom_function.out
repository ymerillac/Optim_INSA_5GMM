Dakota version 6.2 released May 15 2015.
Subversion revision 3394 built Oct 13 2015 16:44:57.
Running serial Dakota executable in serial mode.
Start time: Tue Nov  3 14:23:46 2015

-------------------------------
Begin DAKOTA input file
dakota_input_custom_function.in
-------------------------------
## DAKOTA INPUT FILE - dakota_rosenbrock.in


environment,
	graphics
	tabular_graphics_data
	tabular_graphics_file = 'optim_res.dat'

method,
    #optpp_pds
    #coliny_cobyla
	#coliny_direct
	#hybrid collaborative optpp_q_newton
	#efficient_global
	#optpp_fd_newton
	optpp_q_newton
	#optpp_newton
     	max_iterations = 50           
     	convergence_tolerance = 1e-4 

variables,
	continuous_design = 2
	initial_point  -4.2  4.0
	lower_bounds   -5.0 -5.0	
	upper_bounds    5.0  5.0
	descriptors     'x1' 'x2'

interface,
	system
	analysis_driver = 'python myfunc.py'

responses,
	objective_functions = 1	
	descriptors = 'my_function'
	numerical_gradients
	#no_gradients
	no_hessians
	#numerical_hessians
---------------------
End DAKOTA input file
---------------------

Using Dakota input file 'dakota_input_custom_function.in'
Writing new restart file dakota.rst

>>>>> Executing environment.

>>>>> Running optpp_q_newton iterator.

------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation    1
---------------------
Parameters for evaluation 1:
                     -4.2000000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_875d76de /tmp/dakota_results_cb93acc2

Active response data for evaluation 1:
Active set vector = { 1 }
                      3.3640000000e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    2
---------------------
Parameters for evaluation 2:
                     -4.2042000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_805b1799 /tmp/dakota_results_52834c85

Active response data for evaluation 2:
Active set vector = { 1 }
                      3.3675297640e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    3
---------------------
Parameters for evaluation 3:
                     -4.2000000000e+00 x1
                      4.0040000000e+00 x2

python myfunc.py /tmp/dakota_params_7f8aeda8 /tmp/dakota_results_968707ab

Active response data for evaluation 3:
Active set vector = { 1 }
                      3.3672016000e+01 my_function


>>>>> Total response returned to iterator:

Active set vector = { 3 } Deriv vars vector = { 1 2 }
                      3.3640000000e+01 my_function
 [ -8.4042000000e+00  8.0040000000e+00 ] my_function gradient



---------------------
Begin Evaluation    4
---------------------
Parameters for evaluation 4:
                      4.2041997495e+00 x1
                     -4.0039997615e+00 x2

python myfunc.py /tmp/dakota_params_07c6236f /tmp/dakota_results_3a6ceb67

Active response data for evaluation 4:
Active set vector = { 1 }
                      3.3707309624e+01 my_function



---------------------
Begin Evaluation    5
---------------------
Parameters for evaluation 5:
                      1.0488875031e-06 x1
                     -9.9894077099e-07 x2

python myfunc.py /tmp/dakota_params_1b55d1ff /tmp/dakota_results_06bdfdd5

Active response data for evaluation 5:
Active set vector = { 1 }
                      2.0980476580e-12 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    6
---------------------
Parameters for evaluation 6:
                      1.1048887503e-05 x1
                     -9.9894077099e-07 x2

python myfunc.py /tmp/dakota_params_e6a4bc89 /tmp/dakota_results_3f2ccfd5

Active response data for evaluation 6:
Active set vector = { 1 }
                      1.2307579772e-10 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    7
---------------------
Parameters for evaluation 7:
                      1.0488875031e-06 x1
                     -1.0998940771e-05 x2

python myfunc.py /tmp/dakota_params_c016548f /tmp/dakota_results_efec6cf5

Active response data for evaluation 7:
Active set vector = { 1 }
                      1.2207686308e-10 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  1.2097775006e-05 -1.1997881542e-05 ] my_function gradient


********************************************************
             OPT++ TERMINATION CRITERION                
	  SUCCESS - optpp_q_newton converged to a solution
Algorithm converged - Norm of gradient is less than gradient tolerance
********************************************************
<<<<< Function evaluation summary: 7 total (7 new, 0 duplicate)
<<<<< Best parameters          =
                      1.0488875031e-06 x1
                     -9.9894077099e-07 x2
<<<<< Best objective function  =
                      2.0980476580e-12
<<<<< Best data captured at function evaluation 5


<<<<< Iterator optpp_q_newton completed.
<<<<< Environment execution completed.
DAKOTA execution time in seconds:
  Total CPU        =   0.015143 [parent =      0.012, child =   0.003143]
  Total wall clock =    0.78277
