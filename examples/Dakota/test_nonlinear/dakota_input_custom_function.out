Dakota version 6.2 released May 15 2015.
Subversion revision 3394 built Oct 13 2015 16:44:57.
Running serial Dakota executable in serial mode.
Start time: Tue Nov 17 15:57:09 2015

-------------------------------
Begin DAKOTA input file
dakota_input_custom_function.in
-------------------------------
## DAKOTA INPUT FILE - dakota_rosenbrock.in


environment,
	graphics
	tabular_graphics_data
	tabular_graphics_file = 'optim_res.dat'

method,
    #optpp_pds
    #coliny_cobyla
	#coliny_direct
	#hybrid collaborative optpp_q_newton
	#efficient_global
	#optpp_fd_newton
	optpp_q_newton
	#optpp_newton
     	max_iterations = 50           
     	convergence_tolerance = 1e-4
	

variables,
	continuous_design = 2
	initial_point  4.2  4.0
	
	#lower_bounds   -5.0 -5.0	
	#upper_bounds    5.0  5.0
	descriptors     'x1' 'x2'

interface,
	system
	analysis_driver = 'python myfunc.py'

responses,
	objective_functions = 1	
	nonlinear_equality_constraints 1
	descriptors = 'my_function' 'my_constraint'
	numerical_gradients
	#no_gradients
	no_hessians
	#numerical_hessians
---------------------
End DAKOTA input file
---------------------

Using Dakota input file 'dakota_input_custom_function.in'
Writing new restart file dakota.rst

>>>>> Executing environment.

>>>>> Running optpp_q_newton iterator.

------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation    1
---------------------
Parameters for evaluation 1:
                      4.2000000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_519e4c87 /tmp/dakota_results_d5b73619

Active response data for evaluation 1:
Active set vector = { 1 1 }
                      3.3640000000e+01 my_function
                      2.2000000000e+00 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    2
---------------------
Parameters for evaluation 2:
                      4.2042000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_b51e41ff /tmp/dakota_results_ddf230f7

Active response data for evaluation 2:
Active set vector = { 1 1 }
                      3.3675297640e+01 my_function
                      2.2042000000e+00 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    3
---------------------
Parameters for evaluation 3:
                      4.2000000000e+00 x1
                      4.0040000000e+00 x2

python myfunc.py /tmp/dakota_params_7febeb0f /tmp/dakota_results_6ed8faea

Active response data for evaluation 3:
Active set vector = { 1 1 }
                      3.3672016000e+01 my_function
                      2.2000000000e+00 my_constraint


>>>>> Total response returned to iterator:

Active set vector = { 3 3 } Deriv vars vector = { 1 2 }
                      3.3640000000e+01 my_function
                      2.2000000000e+00 my_constraint
 [  8.4042000000e+00  8.0040000000e+00 ] my_function gradient
 [  1.0000000000e+00  0.0000000000e+00 ] my_constraint gradient



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation    4
---------------------
Parameters for evaluation 4:
                      2.0000000000e+00 x1
                      1.1034482759e+00 x2

python myfunc.py /tmp/dakota_params_83975967 /tmp/dakota_results_2cb710d5

Active response data for evaluation 4:
Active set vector = { 1 1 }
                      5.2175980975e+00 my_function
                     -9.9920072216e-15 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    5
---------------------
Parameters for evaluation 5:
                      2.0020000000e+00 x1
                      1.1034482759e+00 x2

python myfunc.py /tmp/dakota_params_418d59e8 /tmp/dakota_results_221d3e91

Active response data for evaluation 5:
Active set vector = { 1 1 }
                      5.2256020975e+00 my_function
                      2.0000000000e-03 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    6
---------------------
Parameters for evaluation 6:
                      2.0000000000e+00 x1
                      1.1045517241e+00 x2

python myfunc.py /tmp/dakota_params_9cd96128 /tmp/dakota_results_a65efd90

Active response data for evaluation 6:
Active set vector = { 1 1 }
                      5.2200345113e+00 my_function
                     -9.9920072216e-15 my_constraint


>>>>> Total response returned to iterator:

Active set vector = { 3 3 } Deriv vars vector = { 1 2 }
                      5.2175980975e+00 my_function
                     -9.9920072216e-15 my_constraint
 [  4.0020000000e+00  2.2080000063e+00 ] my_function gradient
 [  1.0000000000e+00  0.0000000000e+00 ] my_constraint gradient



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation    7
---------------------
Parameters for evaluation 7:
                      2.0000000000e+00 x1
                      1.3497230270e-01 x2

python myfunc.py /tmp/dakota_params_940fbbae /tmp/dakota_results_98fc3b8d

Active response data for evaluation 7:
Active set vector = { 1 1 }
                      4.0182175225e+00 my_function
                      0.0000000000e+00 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    8
---------------------
Parameters for evaluation 8:
                      2.0020000000e+00 x1
                      1.3497230270e-01 x2

python myfunc.py /tmp/dakota_params_bc3fd998 /tmp/dakota_results_f734ca8e

Active response data for evaluation 8:
Active set vector = { 1 1 }
                      4.0262215225e+00 my_function
                      2.0000000000e-03 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    9
---------------------
Parameters for evaluation 9:
                      2.0000000000e+00 x1
                      1.3510727500e-01 x2

python myfunc.py /tmp/dakota_params_c5ca8a18 /tmp/dakota_results_75c1bd51

Active response data for evaluation 9:
Active set vector = { 1 1 }
                      4.0182539758e+00 my_function
                      0.0000000000e+00 my_constraint


>>>>> Total response returned to iterator:

Active set vector = { 3 3 } Deriv vars vector = { 1 2 }
                      4.0182175225e+00 my_function
                      0.0000000000e+00 my_constraint
 [  4.0020000000e+00  2.7007955908e-01 ] my_function gradient
 [  1.0000000000e+00  0.0000000000e+00 ] my_constraint gradient



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation   10
---------------------
Parameters for evaluation 10:
                      2.0000000000e+00 x1
                      1.1037743525e-08 x2

python myfunc.py /tmp/dakota_params_3600c70d /tmp/dakota_results_14150c95

Active response data for evaluation 10:
Active set vector = { 1 1 }
                      4.0000000000e+00 my_function
                      0.0000000000e+00 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   11
---------------------
Parameters for evaluation 11:
                      2.0020000000e+00 x1
                      1.1037743525e-08 x2

python myfunc.py /tmp/dakota_params_a98490e4 /tmp/dakota_results_45657d05

Active response data for evaluation 11:
Active set vector = { 1 1 }
                      4.0080040000e+00 my_function
                      2.0000000000e-03 my_constraint


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   12
---------------------
Parameters for evaluation 12:
                      2.0000000000e+00 x1
                      1.0011037744e-05 x2

python myfunc.py /tmp/dakota_params_1176735e /tmp/dakota_results_23e1493c

Active response data for evaluation 12:
Active set vector = { 1 1 }
                      4.0000000001e+00 my_function
                      0.0000000000e+00 my_constraint


>>>>> Total response returned to iterator:

Active set vector = { 3 3 } Deriv vars vector = { 1 2 }
                      4.0000000000e+00 my_function
                      0.0000000000e+00 my_constraint
 [  4.0020000000e+00  1.0000000827e-05 ] my_function gradient
 [  1.0000000000e+00  0.0000000000e+00 ] my_constraint gradient


********************************************************
             OPT++ TERMINATION CRITERION                
	  SUCCESS - optpp_q_newton converged to a solution
Algorithm converged - Norm of gradient less than relative gradient tolerance
********************************************************
<<<<< Function evaluation summary: 12 total (12 new, 0 duplicate)
<<<<< Best parameters          =
                      2.0000000000e+00 x1
                      1.1037743525e-08 x2
<<<<< Best objective function  =
                      4.0000000000e+00
<<<<< Best constraint values   =
                      0.0000000000e+00
<<<<< Best data captured at function evaluation 10


<<<<< Iterator optpp_q_newton completed.
<<<<< Environment execution completed.
DAKOTA execution time in seconds:
  Total CPU        =   0.023776 [parent =      0.024, child =  -0.000224]
  Total wall clock =    1.42755
