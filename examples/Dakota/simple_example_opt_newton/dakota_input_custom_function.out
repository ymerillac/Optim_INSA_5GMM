Dakota version 6.2 released May 15 2015.
Subversion revision 3394 built Oct 13 2015 16:44:57.
Running serial Dakota executable in serial mode.
Start time: Wed Nov  4 13:44:51 2015

-------------------------------
Begin DAKOTA input file
dakota_input_custom_function.in
-------------------------------
## DAKOTA INPUT FILE - dakota_rosenbrock.in


environment,
	graphics
	tabular_graphics_data
	tabular_graphics_file = 'optim_res.dat'

method,
    #optpp_pds
    #coliny_cobyla
	#coliny_direct
	#hybrid collaborative optpp_q_newton
	#efficient_global
	#optpp_fd_newton
	#optpp_q_newton
	optpp_newton
     	max_iterations = 50           
     	convergence_tolerance = 1e-4 

variables,
	continuous_design = 2
	initial_point  -4.2  4.0
	lower_bounds   -5.0 -5.0	
	upper_bounds    5.0  5.0
	descriptors     'x1' 'x2'

interface,
	system
	analysis_driver = 'python myfunc.py'

responses,
	objective_functions = 1	
	descriptors = 'my_function'
	numerical_gradients
	#no_gradients
	#no_hessians
	numerical_hessians
---------------------
End DAKOTA input file
---------------------

Using Dakota input file 'dakota_input_custom_function.in'
Writing new restart file dakota.rst

>>>>> Executing environment.

>>>>> Running optpp_newton iterator.

------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation    1
---------------------
Parameters for evaluation 1:
                     -4.2000000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_d2de709a /tmp/dakota_results_854747e1

Active response data for evaluation 1:
Active set vector = { 1 }
                      3.3640000000e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    2
---------------------
Parameters for evaluation 2:
                     -4.2042000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_266d388c /tmp/dakota_results_14a40e55

Active response data for evaluation 2:
Active set vector = { 1 }
                      3.3675297640e+01 my_function


>>>>> Dakota finite difference Hessian evaluation for x[1] + 2h:

---------------------
Begin Evaluation    3
---------------------
Parameters for evaluation 3:
                     -4.2168000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_832583ec /tmp/dakota_results_4a26c9d6

Active response data for evaluation 3:
Active set vector = { 1 }
                      3.3781402240e+01 my_function


>>>>> Dakota finite difference Hessian evaluation for x[1] - 2h:

---------------------
Begin Evaluation    4
---------------------
Parameters for evaluation 4:
                     -4.1832000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_84bdb0f4 /tmp/dakota_results_4d35fe84

Active response data for evaluation 4:
Active set vector = { 1 }
                      3.3499162240e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    5
---------------------
Parameters for evaluation 5:
                     -4.2000000000e+00 x1
                      4.0040000000e+00 x2

python myfunc.py /tmp/dakota_params_6d281dd4 /tmp/dakota_results_296b2a35

Active response data for evaluation 5:
Active set vector = { 1 }
                      3.3672016000e+01 my_function


>>>>> Dakota finite difference Hessian evaluation for x[2] + 2h:

---------------------
Begin Evaluation    6
---------------------
Parameters for evaluation 6:
                     -4.2000000000e+00 x1
                      4.0160000000e+00 x2

python myfunc.py /tmp/dakota_params_bec32e38 /tmp/dakota_results_ab641d87

Active response data for evaluation 6:
Active set vector = { 1 }
                      3.3768256000e+01 my_function


>>>>> Dakota finite difference Hessian evaluation for x[2] - 2h:

---------------------
Begin Evaluation    7
---------------------
Parameters for evaluation 7:
                     -4.2000000000e+00 x1
                      3.9840000000e+00 x2

python myfunc.py /tmp/dakota_params_ddb6985e /tmp/dakota_results_85a8b028

Active response data for evaluation 7:
Active set vector = { 1 }
                      3.3512256000e+01 my_function


>>>>> Dakota finite difference Hessian evaluation for x[2] + h, x[1] + h:

---------------------
Begin Evaluation    8
---------------------
Parameters for evaluation 8:
                     -4.2168000000e+00 x1
                      4.0160000000e+00 x2

python myfunc.py /tmp/dakota_params_14fefac6 /tmp/dakota_results_10ca830b

Active response data for evaluation 8:
Active set vector = { 1 }
                      3.3909658240e+01 my_function


>>>>> Total response returned to iterator:

Active set vector = { 7 } Deriv vars vector = { 1 2 }
                      3.3640000000e+01 my_function
 [ -8.4042000000e+00  8.0040000000e+00 ] my_function gradient
[[  2.0000000000e+00 -0.0000000000e+00 
   -0.0000000000e+00  2.0000000000e+00 ]] my_function Hessian



---------------------
Begin Evaluation    9
---------------------
Parameters for evaluation 9:
                      2.0998748103e-03 x1
                     -1.9998807269e-03 x2

python myfunc.py /tmp/dakota_params_5dad4a31 /tmp/dakota_results_d760eb75

Active response data for evaluation 9:
Active set vector = { 1 }
                      8.4089971411e-06 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   10
---------------------
Parameters for evaluation 10:
                      2.1098748103e-03 x1
                     -1.9998807269e-03 x2

python myfunc.py /tmp/dakota_params_675ad24b /tmp/dakota_results_c6c8ce03

Active response data for evaluation 10:
Active set vector = { 1 }
                      8.4510946373e-06 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   11
---------------------
Parameters for evaluation 11:
                      2.0998748103e-03 x1
                     -2.0098807269e-03 x2

python myfunc.py /tmp/dakota_params_a44d5e69 /tmp/dakota_results_c0f3ce2e

Active response data for evaluation 11:
Active set vector = { 1 }
                      8.4490947556e-06 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  4.2097496210e-03 -4.0097614540e-03 ] my_function gradient



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference Hessian evaluation for x[1] + 2h:

---------------------
Begin Evaluation   12
---------------------
Parameters for evaluation 12:
                      2.1398748103e-03 x1
                     -1.9998807269e-03 x2

python myfunc.py /tmp/dakota_params_1774d804 /tmp/dakota_results_abaa5152

Active response data for evaluation 12:
Active set vector = { 1 }
                      8.5785871259e-06 my_function


>>>>> Dakota finite difference Hessian evaluation for x[1] - 2h:

---------------------
Begin Evaluation   13
---------------------
Parameters for evaluation 13:
                      2.0598748103e-03 x1
                     -1.9998807269e-03 x2

python myfunc.py /tmp/dakota_params_0f777499 /tmp/dakota_results_acf1a2c8

Active response data for evaluation 13:
Active set vector = { 1 }
                      8.2426071562e-06 my_function


>>>>> Dakota finite difference Hessian evaluation for x[2] + 2h:

---------------------
Begin Evaluation   14
---------------------
Parameters for evaluation 14:
                      2.0998748103e-03 x1
                     -2.0398807269e-03 x2

python myfunc.py /tmp/dakota_params_46f6d87d /tmp/dakota_results_2dfa87ee

Active response data for evaluation 14:
Active set vector = { 1 }
                      8.5705875992e-06 my_function


>>>>> Dakota finite difference Hessian evaluation for x[2] - 2h:

---------------------
Begin Evaluation   15
---------------------
Parameters for evaluation 15:
                      2.0998748103e-03 x1
                     -1.9598807269e-03 x2

python myfunc.py /tmp/dakota_params_7fb6638e /tmp/dakota_results_1b0dcf80

Active response data for evaluation 15:
Active set vector = { 1 }
                      8.2506066829e-06 my_function


>>>>> Dakota finite difference Hessian evaluation for x[2] + h, x[1] + h:

---------------------
Begin Evaluation   16
---------------------
Parameters for evaluation 16:
                      2.1398748103e-03 x1
                     -2.0398807269e-03 x2

python myfunc.py /tmp/dakota_params_76842ddc /tmp/dakota_results_cd3959eb

Active response data for evaluation 16:
Active set vector = { 1 }
                      8.7401775840e-06 my_function


>>>>> Total response returned to iterator:

Active set vector = { 4 } Deriv vars vector = { 1 2 }
[[  2.0000000062e+00  6.2489855684e-09 
    6.2489855684e-09  2.0000000062e+00 ]] my_function Hessian



---------------------
Begin Evaluation   17
---------------------
Parameters for evaluation 17:
                     -4.9999371287e-06 x1
                      4.9999406160e-06 x2

python myfunc.py /tmp/dakota_params_183d0e6e /tmp/dakota_results_c8efbd93

Active response data for evaluation 17:
Active set vector = { 1 }
                      4.9998777455e-11 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   18
---------------------
Parameters for evaluation 18:
                     -1.4999937129e-05 x1
                      4.9999406160e-06 x2

python myfunc.py /tmp/dakota_params_dd6a43a4 /tmp/dakota_results_f038bf73

Active response data for evaluation 18:
Active set vector = { 1 }
                      2.4999752003e-10 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   19
---------------------
Parameters for evaluation 19:
                     -4.9999371287e-06 x1
                      1.4999940616e-05 x2

python myfunc.py /tmp/dakota_params_e11025e5 /tmp/dakota_results_d9b22413

Active response data for evaluation 19:
Active set vector = { 1 }
                      2.4999758978e-10 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [ -1.9999874257e-05  1.9999881232e-05 ] my_function gradient


********************************************************
             OPT++ TERMINATION CRITERION                
	  SUCCESS - optpp_newton converged to a solution
Algorithm converged - Difference in successive fcn values less than tolerance
********************************************************
<<<<< Function evaluation summary: 19 total (19 new, 0 duplicate)
<<<<< Best parameters          =
                     -4.9999371287e-06 x1
                      4.9999406160e-06 x2
<<<<< Best objective function  =
                      4.9998777455e-11
<<<<< Best data captured at function evaluation 17


<<<<< Iterator optpp_newton completed.
<<<<< Environment execution completed.
DAKOTA execution time in seconds:
  Total CPU        =   0.032607 [parent =      0.032, child =   0.000607]
  Total wall clock =    3.58899
