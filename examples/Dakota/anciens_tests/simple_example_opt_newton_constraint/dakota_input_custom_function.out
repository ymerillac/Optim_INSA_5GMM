Dakota version 6.2 released May 15 2015.
Subversion revision 3394 built Oct 13 2015 16:44:57.
Running serial Dakota executable in serial mode.
Start time: Tue Nov 17 12:23:10 2015

-------------------------------
Begin DAKOTA input file
dakota_input_custom_function.in
-------------------------------
## DAKOTA INPUT FILE - dakota_rosenbrock.in


environment,
	graphics
	tabular_graphics_data
	tabular_graphics_file = 'optim_res.dat'

method,
    #optpp_pds
    #coliny_cobyla
	#coliny_direct
	#hybrid collaborative optpp_q_newton
	#efficient_global
	#optpp_fd_newton
	optpp_q_newton
	#optpp_newton
     	max_iterations = 50           
     	convergence_tolerance = 1e-4 
	linear_inequality_constraint_matrix 1 0
	0 1
	linear_equality_constraint_matrix 2 -1
	linear_inequality_upper_bounds nan nan
	linear_inequality_lower_bounds 1 1
	linear_equality_targets 2
	

variables,
	continuous_design = 2
	initial_point  4.2  4.0
	
	#lower_bounds   -5.0 -5.0	
	#upper_bounds    5.0  5.0
	descriptors     'x1' 'x2'

interface,
	system
	analysis_driver = 'python myfunc.py'

responses,
	objective_functions = 1	
	#nonlinear_equality_constraints 1 c'est la que ca coince
	descriptors = 'my_function'
	numerical_gradients
	#no_gradients
	no_hessians
	#numerical_hessians
---------------------
End DAKOTA input file
---------------------

Using Dakota input file 'dakota_input_custom_function.in'
Writing new restart file dakota.rst

>>>>> Executing environment.

>>>>> Running optpp_q_newton iterator.

------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> Initial map for analytic portion of response:

---------------------
Begin Evaluation    1
---------------------
Parameters for evaluation 1:
                      4.2000000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_70dc4a39 /tmp/dakota_results_bf64ed50

Active response data for evaluation 1:
Active set vector = { 1 }
                      3.3640000000e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    2
---------------------
Parameters for evaluation 2:
                      4.2042000000e+00 x1
                      4.0000000000e+00 x2

python myfunc.py /tmp/dakota_params_2d54241d /tmp/dakota_results_1ff4f26a

Active response data for evaluation 2:
Active set vector = { 1 }
                      3.3675297640e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    3
---------------------
Parameters for evaluation 3:
                      4.2000000000e+00 x1
                      4.0040000000e+00 x2

python myfunc.py /tmp/dakota_params_0d7fb89f /tmp/dakota_results_348eaa33

Active response data for evaluation 3:
Active set vector = { 1 }
                      3.3672016000e+01 my_function


>>>>> Total response returned to iterator:

Active set vector = { 3 } Deriv vars vector = { 1 2 }
                      3.3640000000e+01 my_function
 [  8.4042000000e+00  8.0040000000e+00 ] my_function gradient



---------------------
Begin Evaluation    4
---------------------
Parameters for evaluation 4:
                      2.7053739552e+00 x1
                      3.4107479104e+00 x2

python myfunc.py /tmp/dakota_params_9e3a2723 /tmp/dakota_results_919b7a2e

Active response data for evaluation 4:
Active set vector = { 1 }
                      1.8952249546e+01 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    5
---------------------
Parameters for evaluation 5:
                      2.7080793292e+00 x1
                      3.4107479104e+00 x2

python myfunc.py /tmp/dakota_params_51b919c6 /tmp/dakota_results_073ba7f6

Active response data for evaluation 5:
Active set vector = { 1 }
                      1.8966894961e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    6
---------------------
Parameters for evaluation 6:
                      2.7053739552e+00 x1
                      3.4141586583e+00 x2

python myfunc.py /tmp/dakota_params_5b0ca901 /tmp/dakota_results_f891445c

Active response data for evaluation 6:
Active set vector = { 1 }
                      1.8975527582e+01 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  5.4134532758e+00  6.8249065634e+00 ] my_function gradient



---------------------
Begin Evaluation    7
---------------------
Parameters for evaluation 7:
                      2.3350769005e+00 x1
                      2.6701538010e+00 x2

python myfunc.py /tmp/dakota_params_99e02619 /tmp/dakota_results_bf6fcf4d

Active response data for evaluation 7:
Active set vector = { 1 }
                      1.2582305452e+01 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation    8
---------------------
Parameters for evaluation 8:
                      2.3374119774e+00 x1
                      2.6701538010e+00 x2

python myfunc.py /tmp/dakota_params_a4454707 /tmp/dakota_results_50272de2

Active response data for evaluation 8:
Active set vector = { 1 }
                      1.2593216073e+01 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation    9
---------------------
Parameters for evaluation 9:
                      2.3350769005e+00 x1
                      2.6728239548e+00 x2

python myfunc.py /tmp/dakota_params_05627586 /tmp/dakota_results_ea6d9a91

Active response data for evaluation 9:
Active set vector = { 1 }
                      1.2596572025e+01 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  4.6724888579e+00  5.3429777321e+00 ] my_function gradient



---------------------
Begin Evaluation   10
---------------------
Parameters for evaluation 10:
                      1.7264200576e+00 x1
                      1.4528401153e+00 x2

python myfunc.py /tmp/dakota_params_49663cb2 /tmp/dakota_results_55276e06

Active response data for evaluation 10:
Active set vector = { 1 }
                      5.0912706159e+00 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   11
---------------------
Parameters for evaluation 11:
                      1.7281464777e+00 x1
                      1.4528401153e+00 x2

python myfunc.py /tmp/dakota_params_1bb59469 /tmp/dakota_results_6f793167

Active response data for evaluation 11:
Active set vector = { 1 }
                      5.0972346489e+00 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   12
---------------------
Parameters for evaluation 12:
                      1.7264200576e+00 x1
                      1.4542929554e+00 x2

python myfunc.py /tmp/dakota_params_90c98e27 /tmp/dakota_results_2db8206b

Active response data for evaluation 12:
Active set vector = { 1 }
                      5.0954942154e+00 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  3.4545665371e+00  2.9071330738e+00 ] my_function gradient



---------------------
Begin Evaluation   13
---------------------
Parameters for evaluation 13:
                      1.5187345441e+00 x1
                      1.0374690881e+00 x2

python myfunc.py /tmp/dakota_params_c43a0344 /tmp/dakota_results_3995d2c9

Active response data for evaluation 13:
Active set vector = { 1 }
                      3.3828967241e+00 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   14
---------------------
Parameters for evaluation 14:
                      1.5202532786e+00 x1
                      1.0374690881e+00 x2

python myfunc.py /tmp/dakota_params_dbc29489 /tmp/dakota_results_d27a1dab

Active response data for evaluation 14:
Active set vector = { 1 }
                      3.3875121399e+00 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   15
---------------------
Parameters for evaluation 15:
                      1.5187345441e+00 x1
                      1.0385065572e+00 x2

python myfunc.py /tmp/dakota_params_5689fda1 /tmp/dakota_results_976e600a

Active response data for evaluation 15:
Active set vector = { 1 }
                      3.3850504847e+00 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  3.0389878192e+00  2.0759756456e+00 ] my_function gradient



---------------------
Begin Evaluation   16
---------------------
Parameters for evaluation 16:
                      1.5095192382e+00 x1
                      1.0190384764e+00 x2

python myfunc.py /tmp/dakota_params_811fe79f /tmp/dakota_results_c1f6a31f

Active response data for evaluation 16:
Active set vector = { 1 }
                      3.3170877470e+00 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   17
---------------------
Parameters for evaluation 17:
                      1.5110287575e+00 x1
                      1.0190384764e+00 x2

python myfunc.py /tmp/dakota_params_a3a3f652 /tmp/dakota_results_38a413f9

Active response data for evaluation 17:
Active set vector = { 1 }
                      3.3216473223e+00 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   18
---------------------
Parameters for evaluation 18:
                      1.5095192382e+00 x1
                      1.0200575149e+00 x2

python myfunc.py /tmp/dakota_params_b100bcce /tmp/dakota_results_1498f1e5

Active response data for evaluation 18:
Active set vector = { 1 }
                      3.3191656643e+00 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  3.0205479960e+00  2.0390959890e+00 ] my_function gradient



---------------------
Begin Evaluation   19
---------------------
Parameters for evaluation 19:
                      1.5000280508e+00 x1
                      1.0000561016e+00 x2

python myfunc.py /tmp/dakota_params_cc5f466c /tmp/dakota_results_e381d700

Active response data for evaluation 19:
Active set vector = { 1 }
                      3.2501963596e+00 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   20
---------------------
Parameters for evaluation 20:
                      1.5015280789e+00 x1
                      1.0000561016e+00 x2

python myfunc.py /tmp/dakota_params_1239651e /tmp/dakota_results_57289a4e

Active response data for evaluation 20:
Active set vector = { 1 }
                      3.2546987780e+00 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   21
---------------------
Parameters for evaluation 21:
                      1.5000280508e+00 x1
                      1.0010561577e+00 x2

python myfunc.py /tmp/dakota_params_54abdca7 /tmp/dakota_results_e4b7c0ad

Active response data for evaluation 21:
Active set vector = { 1 }
                      3.2521975841e+00 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  3.0015561293e+00  2.0011122544e+00 ] my_function gradient



---------------------
Begin Evaluation   22
---------------------
Parameters for evaluation 22:
                      1.5001027873e+00 x1
                      1.0002055745e+00 x2

python myfunc.py /tmp/dakota_params_d500c4f8 /tmp/dakota_results_92b81f1f

Active response data for evaluation 22:
Active set vector = { 1 }
                      3.2507195637e+00 my_function



------------------------------------------
Begin Dakota derivative estimation routine
------------------------------------------

>>>>> map at X performed previously and results retrieved

>>>>> Dakota finite difference gradient evaluation for x[1] + h:

---------------------
Begin Evaluation   23
---------------------
Parameters for evaluation 23:
                      1.5016028900e+00 x1
                      1.0002055745e+00 x2

python myfunc.py /tmp/dakota_params_8f594d5d /tmp/dakota_results_82f249bd

Active response data for evaluation 23:
Active set vector = { 1 }
                      3.2552224307e+00 my_function


>>>>> Dakota finite difference gradient evaluation for x[2] + h:

---------------------
Begin Evaluation   24
---------------------
Parameters for evaluation 24:
                      1.5001027873e+00 x1
                      1.0012057801e+00 x2

python myfunc.py /tmp/dakota_params_61cd16d6 /tmp/dakota_results_3a796770

Active response data for evaluation 24:
Active set vector = { 1 }
                      3.2527213865e+00 my_function


>>>>> Total response returned to iterator:

Active set vector = { 2 } Deriv vars vector = { 1 2 }
 [  3.0017056753e+00  2.0014113508e+00 ] my_function gradient


********************************************************
             OPT++ TERMINATION CRITERION                
	  SUCCESS - optpp_q_newton converged to a solution
Algorithm converged - Norm of gradient less than relative gradient tolerance
********************************************************
<<<<< Function evaluation summary: 24 total (24 new, 0 duplicate)
<<<<< Best parameters          =
                      1.5001027873e+00 x1
                      1.0002055745e+00 x2
<<<<< Best objective function  =
                      3.2507195637e+00
<<<<< Best data captured at function evaluation 22


<<<<< Iterator optpp_q_newton completed.
<<<<< Environment execution completed.
DAKOTA execution time in seconds:
  Total CPU        =   0.045238 [parent =      0.044, child =   0.001238]
  Total wall clock =    3.41963
